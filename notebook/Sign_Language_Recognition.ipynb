{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TDrA__xKekdW",
        "outputId": "515ef4f9-505f-4b5a-8d26-dd206e98c76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.18 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 48"
      ],
      "metadata": {
        "id": "mrgoDzjvewuD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 2"
      ],
      "metadata": {
        "id": "cCM45wB3l73-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video processing"
      ],
      "metadata": {
        "id": "Wmm3bx11fDds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data augmentation"
      ],
      "metadata": {
        "id": "eZqhwFotfwmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "sPqG--Xgf7sP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_flip(image):\n",
        "    \"\"\"Apply flipping for image\"\"\"\n",
        "    return cv2.flip(image, 1)\n",
        "\n",
        "def apply_rotation(image, angle):\n",
        "    \"\"\"Apply rotation with given angle\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    return cv2.warpAffine(image, matrix, (w, h))\n",
        "\n",
        "def apply_color_shift(image, value):\n",
        "    \"\"\"Apply color shifting with given value\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hsv[..., 2] = cv2.add(hsv[..., 2], value)\n",
        "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)"
      ],
      "metadata": {
        "id": "3QoSJY0Hf82t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Extract Frame From Video"
      ],
      "metadata": {
        "id": "zzUAB_KjgFYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vidToFrame(vid_file, base_out_dir, sequence_length):\n",
        "    # Open the video file\n",
        "    video_capture = cv2.VideoCapture(vid_file)\n",
        "    success, frame_count = True, 0\n",
        "\n",
        "    # Create a directory to store the original frames\n",
        "    os.makedirs(base_out_dir, exist_ok=True)\n",
        "\n",
        "    # Create directories for each effect (flip, rotation, color shift)\n",
        "    effect_dirs = {\n",
        "        \"flip\": f\"{base_out_dir}_flip\",\n",
        "        \"rotation\": f\"{base_out_dir}_rotation\",\n",
        "        \"color_shift\": f\"{base_out_dir}_color_shift\"\n",
        "    }\n",
        "    for effect_dir in effect_dirs.values():\n",
        "        os.makedirs(effect_dir, exist_ok=True)\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the interval between frames to ensure sequence_length frames are captured\n",
        "    frame_interval = max(int(total_frames / sequence_length), 1)\n",
        "\n",
        "    # Parameters for effects\n",
        "    angle = random.randint(-30, 30)  # Random rotation angle\n",
        "    color_shift_value = random.randint(-50, 50)  # Random color shift value\n",
        "\n",
        "    print(f\"Applying effects: flip, rotation (angle={angle}), color shift (value={color_shift_value})\")\n",
        "\n",
        "    # Extract frames and apply effects\n",
        "    while frame_count < sequence_length:\n",
        "        # Set the position in the video to the desired frame\n",
        "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n",
        "        success, frame = video_capture.read()\n",
        "        if not success or frame is None:  # Stop if the frame cannot be read\n",
        "            break\n",
        "        frame_count += 1\n",
        "\n",
        "        # Save the original frame\n",
        "        original_path = os.path.join(base_out_dir, f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(original_path, frame)\n",
        "\n",
        "        # Apply and save each effect\n",
        "        flip_frame = apply_flip(frame)\n",
        "        flip_path = os.path.join(effect_dirs[\"flip\"], f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(flip_path, flip_frame)\n",
        "\n",
        "        rotated_frame = apply_rotation(frame, angle)\n",
        "        rotation_path = os.path.join(effect_dirs[\"rotation\"], f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(rotation_path, rotated_frame)\n",
        "\n",
        "        color_shifted_frame = apply_color_shift(frame, color_shift_value)\n",
        "        color_shift_path = os.path.join(effect_dirs[\"color_shift\"], f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(color_shift_path, color_shifted_frame)\n",
        "\n",
        "    # Release the video file resources\n",
        "    video_capture.release()\n",
        "\n",
        "# Paths to dataset and videos\n",
        "file_path = '/content/sign-language-dataset/WLASL_v0.3.json'\n",
        "missing_file_path = '/content/sign-language-dataset/missing.txt'\n",
        "videos_dir = '/content/sign-language-dataset/videos/'\n",
        "\n",
        "# Load the WLASL dataset\n",
        "with open(file_path) as file:\n",
        "    wlasl = json.load(file)\n",
        "\n",
        "# Read the list of missing videos\n",
        "with open(missing_file_path, 'r') as file:\n",
        "    missing_videos = file.read().splitlines()\n",
        "\n",
        "# Create the dataset directory\n",
        "dataset_dir = '/content/datasets'\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Process each class in the dataset\n",
        "for i, class_data in enumerate(wlasl):\n",
        "    if i >= num_words:  # Limit processing to a certain number of classes\n",
        "        break\n",
        "    class_name = class_data['gloss']\n",
        "    print(f\"Processing class: {class_name}, {i}\")\n",
        "\n",
        "    # Process each instance (video) for the current class\n",
        "    for instance in class_data['instances']:\n",
        "        video_id = instance['video_id']\n",
        "        if video_id not in missing_videos:  # Skip missing videos\n",
        "            video_file = os.path.join(videos_dir, f\"{video_id}.mp4\")\n",
        "            split_dir = 'Train' if instance['split'] == 'train' else 'Test'\n",
        "            output_dir = os.path.join(dataset_dir, split_dir, class_name, video_id)\n",
        "            vidToFrame(video_file, output_dir, sequence_length)  # Convert video to frames\n",
        "            print(f\"Processed video {video_id} ({split_dir})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pd9cL7Ioe939",
        "outputId": "2fed1f16-248a-4b81-b00d-23e4a9e6644c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing class: book, 0\n",
            "Applying effects: flip, rotation (angle=-12), color shift (value=-2)\n",
            "Processed video 69241 (Train)\n",
            "Applying effects: flip, rotation (angle=20), color shift (value=33)\n",
            "Processed video 07069 (Train)\n",
            "Applying effects: flip, rotation (angle=-24), color shift (value=-44)\n",
            "Processed video 07068 (Train)\n",
            "Applying effects: flip, rotation (angle=8), color shift (value=-42)\n",
            "Processed video 07070 (Train)\n",
            "Applying effects: flip, rotation (angle=22), color shift (value=25)\n",
            "Processed video 07099 (Test)\n",
            "Applying effects: flip, rotation (angle=0), color shift (value=14)\n",
            "Processed video 07074 (Train)\n",
            "Processing class: drink, 1\n",
            "Applying effects: flip, rotation (angle=-3), color shift (value=22)\n",
            "Processed video 69302 (Test)\n",
            "Applying effects: flip, rotation (angle=24), color shift (value=-14)\n",
            "Processed video 65539 (Train)\n",
            "Applying effects: flip, rotation (angle=-20), color shift (value=-18)\n",
            "Processed video 17710 (Train)\n",
            "Applying effects: flip, rotation (angle=30), color shift (value=28)\n",
            "Processed video 17733 (Train)\n",
            "Applying effects: flip, rotation (angle=-21), color shift (value=-2)\n",
            "Processed video 65540 (Train)\n",
            "Applying effects: flip, rotation (angle=30), color shift (value=-24)\n",
            "Processed video 17734 (Test)\n",
            "Applying effects: flip, rotation (angle=-4), color shift (value=25)\n",
            "Processed video 17711 (Train)\n",
            "Applying effects: flip, rotation (angle=-25), color shift (value=20)\n",
            "Processed video 17712 (Train)\n",
            "Applying effects: flip, rotation (angle=-20), color shift (value=45)\n",
            "Processed video 17713 (Test)\n",
            "Applying effects: flip, rotation (angle=5), color shift (value=-45)\n",
            "Processed video 17709 (Train)\n",
            "Applying effects: flip, rotation (angle=1), color shift (value=-27)\n",
            "Processed video 17720 (Train)\n",
            "Applying effects: flip, rotation (angle=-17), color shift (value=-25)\n",
            "Processed video 17721 (Train)\n",
            "Applying effects: flip, rotation (angle=22), color shift (value=42)\n",
            "Processed video 17722 (Train)\n",
            "Applying effects: flip, rotation (angle=-9), color shift (value=3)\n",
            "Processed video 17723 (Train)\n",
            "Applying effects: flip, rotation (angle=22), color shift (value=-30)\n",
            "Processed video 17724 (Test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Keypoint extraction"
      ],
      "metadata": {
        "id": "1WZW0DIUfLwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mediapipe models and utilities\n",
        "mp_holistic = mp.solutions.holistic  # Mediapipe holistic model for detecting pose, face, and hands\n",
        "mp_drawing = mp.solutions.drawing_utils  # Utility for drawing landmarks\n",
        "mp_drawing_styles = mp.solutions.drawing_styles  # Utility for styling landmarks\n",
        "\n",
        "# Function to detect and draw landmarks from a video frame using Mediapipe\n",
        "def mediapipe_detection(image, model):\n",
        "    # Convert the image from BGR (OpenCV default) to RGB (Mediapipe requirement)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image.flags.writeable = False  # Mark image as read-only to improve performance\n",
        "    results = model.process(image)  # Process the image to detect landmarks\n",
        "    image.flags.writeable = True  # Allow image modifications again\n",
        "    # Convert the image back to BGR for OpenCV compatibility\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    return image, results  # Return the processed image and detection results\n",
        "\n",
        "# Function to extract keypoints from Mediapipe detection results\n",
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
        "    return np.concatenate([pose, face, lh, rh])\n"
      ],
      "metadata": {
        "id": "F23N9B5kjEBo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import cv2\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "VZM-pUqlfZ1t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Loop through all frames"
      ],
      "metadata": {
        "id": "-P-VtL64lWjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_keypoint_arrays(path, split):\n",
        "    # Get the list of all subdirectories (words) in the specified split folder\n",
        "    selected_words = os.listdir(os.path.join(path, split))\n",
        "\n",
        "    # Create base directories for storing npy arrays\n",
        "    os.makedirs('/content/datasets/npy_arrays', exist_ok=True)\n",
        "    os.makedirs(f'/content/datasets/npy_arrays/{split}', exist_ok=True)\n",
        "    working_path = f'/content/datasets/npy_arrays/{split}'\n",
        "\n",
        "    # Path to the folder containing word subfolders\n",
        "    words_folder = os.path.join(path, split)\n",
        "    selected_words1 = []\n",
        "\n",
        "    # Filter words that have not been processed (not present in the npy folder)\n",
        "    for words1 in selected_words:\n",
        "        npy_fold = os.listdir(working_path)\n",
        "        if words1 not in npy_fold:\n",
        "            selected_words1.append(words1)\n",
        "\n",
        "    # Process each word folder that hasn't been processed yet\n",
        "    for word in tqdm(selected_words1):\n",
        "        word_path = os.path.join(working_path, word)\n",
        "        os.makedirs(word_path, exist_ok=True)\n",
        "        video_files = os.listdir(os.path.join(words_folder, word))\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(word_path, video_file)\n",
        "            os.makedirs(video_path, exist_ok=True)\n",
        "            video = sorted(os.listdir(os.path.join(words_folder, word, video_file)))\n",
        "\n",
        "            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "                frame_count = 0\n",
        "                for frame in video:\n",
        "                    frame_path = os.path.join(words_folder, word, video_file, frame)\n",
        "                    frame = cv2.imread(frame_path)\n",
        "                    image, results = mediapipe_detection(frame, holistic)\n",
        "                    keypoints = extract_keypoints(results)\n",
        "                    npy_file_path = os.path.join(video_path, f'{frame_count}.npy')\n",
        "                    np.save(npy_file_path, keypoints)\n",
        "\n",
        "                    frame_count += 1\n",
        "\n",
        "            # Pad the remaining frames with zero arrays if the sequence length is not reached\n",
        "            while frame_count < sequence_length:\n",
        "                npy_file_path = os.path.join(video_path, f'{frame_count}.npy')\n",
        "                np.save(npy_file_path, np.zeros(1662))\n",
        "                frame_count += 1"
      ],
      "metadata": {
        "id": "HRrrzxvnfddX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_keypoint_arrays(f'{dataset_dir}','Train/')"
      ],
      "metadata": {
        "id": "TH4KMzUBfjYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0cc1db-150f-40e9-b8fc-43090932fd6a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [04:56<00:00, 148.06s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_keypoint_arrays(f'{dataset_dir}','Test')"
      ],
      "metadata": {
        "id": "lYe-XhrLfj1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Preprocess Data and Create Labels and Features"
      ],
      "metadata": {
        "id": "qU_EI5Gi1nWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "words = np.array(os.listdir('/content/datasets/Train'))\n",
        "print(words)\n",
        "label_map = {label: num for num, label in enumerate(words)}\n",
        "print(label_map)\n",
        "\n",
        "sequences = []\n",
        "labels = []\n",
        "\n",
        "# Loop through each word in the list of words\n",
        "for word in words:\n",
        "    DATA_PATH = os.path.join('/content/datasets/npy_arrays/Train', word)\n",
        "    videos = os.listdir(DATA_PATH)\n",
        "\n",
        "    # Loop through each video folder in the current word's directory\n",
        "    for video in videos:\n",
        "        # List all sequence files in the current video directory\n",
        "        for sequence in np.array(os.listdir(os.path.join(DATA_PATH, video))).astype(str):\n",
        "            window = []\n",
        "\n",
        "            # Loop through a fixed number of sequence frames (sequence_length is predefined)\n",
        "            for frame_num in range(sequence_length):\n",
        "                # Load the .npy file corresponding to the current frame number\n",
        "                res = np.load(os.path.join(DATA_PATH, video, \"{}.npy\".format(frame_num)))\n",
        "                window.append(res)\n",
        "\n",
        "            sequences.append(window)\n",
        "            labels.append(label_map[word])\n",
        "\n",
        "# Convert sequences and labels into numpy arrays for compatibility with machine learning libraries\n",
        "np.array(sequences).shape\n",
        "np.array(labels).shape\n",
        "X = np.array(sequences)\n",
        "X.shape\n",
        "y = to_categorical(labels).astype(int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
        "y_test.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YeZc6j36q7r",
        "outputId": "ed23ad35-646a-4d61-8754-1a6bd82344da"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book' 'drink']\n",
            "{'book': 0, 'drink': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(154, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Build and Train LSTM Neural Network"
      ],
      "metadata": {
        "id": "pL6rh4p51pnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir=log_dir)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(words.shape[0], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "W-YN_IH41s_D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "7cc730af-e32b-4b15-ace7-a2b49ccad96f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 209ms/step - categorical_accuracy: 0.6377 - loss: 451.1283\n",
            "Epoch 2/200\n",
            "\u001b[1m 7/92\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 312ms/step - categorical_accuracy: 0.5948 - loss: 587185.4375"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9a5c4686ba61>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtb_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}