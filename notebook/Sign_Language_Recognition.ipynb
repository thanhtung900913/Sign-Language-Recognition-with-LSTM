{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Download the WLASL Datasets"
      ],
      "metadata": {
        "id": "yqhfF8X5_KV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "5-qnD0MV_Ide"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "kzvLBri7_SJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d risangbaskoro/wlasl-processed\n",
        "!unzip wlasl-processed.zip -d ./sign-language-dataset"
      ],
      "metadata": {
        "id": "dQQLBm7w_V-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDrA__xKekdW"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Set the length of sequence(FPS), you can adjust it as you want."
      ],
      "metadata": {
        "id": "n8KY-4sWAA-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 48"
      ],
      "metadata": {
        "id": "mrgoDzjvewuD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Number of words you want to process"
      ],
      "metadata": {
        "id": "OHIMXij4ARSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 2000"
      ],
      "metadata": {
        "id": "cCM45wB3l73-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video processing"
      ],
      "metadata": {
        "id": "Wmm3bx11fDds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data augmentation"
      ],
      "metadata": {
        "id": "eZqhwFotfwmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "sPqG--Xgf7sP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_flip(image):\n",
        "    \"\"\"Apply flipping for image\"\"\"\n",
        "    return cv2.flip(image, 1)\n",
        "\n",
        "def apply_rotation(image, angle):\n",
        "    \"\"\"Apply rotation with given angle\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    return cv2.warpAffine(image, matrix, (w, h))\n",
        "\n",
        "def apply_color_shift(image, value):\n",
        "    \"\"\"Apply color shifting with given value\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hsv[..., 2] = cv2.add(hsv[..., 2], value)\n",
        "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)"
      ],
      "metadata": {
        "id": "3QoSJY0Hf82t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Extract Frame From Video"
      ],
      "metadata": {
        "id": "zzUAB_KjgFYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vidToFrame(vid_file, base_out_dir, sequence_length):\n",
        "    # Open the video file\n",
        "    video_capture = cv2.VideoCapture(vid_file)\n",
        "    success, frame_count = True, 0\n",
        "\n",
        "    # Create a directory to store the original frames\n",
        "    os.makedirs(base_out_dir, exist_ok=True)\n",
        "\n",
        "    # Create directories for each effect (flip, rotation, color shift)\n",
        "    effect_dirs = {\n",
        "        \"flip\": f\"{base_out_dir}_flip\",\n",
        "        \"rotation\": f\"{base_out_dir}_rotation\",\n",
        "        \"color_shift\": f\"{base_out_dir}_color_shift\"\n",
        "    }\n",
        "    for effect_dir in effect_dirs.values():\n",
        "        os.makedirs(effect_dir, exist_ok=True)\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the interval between frames to ensure sequence_length frames are captured\n",
        "    frame_interval = max(int(total_frames / sequence_length), 1)\n",
        "\n",
        "    # Parameters for effects\n",
        "    angle = random.randint(-30, 30)  # Random rotation angle\n",
        "    color_shift_value = random.randint(-50, 50)  # Random color shift value\n",
        "\n",
        "    print(f\"Applying effects: flip, rotation (angle={angle}), color shift (value={color_shift_value})\")\n",
        "\n",
        "    # Extract frames and apply effects\n",
        "    while frame_count < sequence_length:\n",
        "        # Set the position in the video to the desired frame\n",
        "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n",
        "        success, frame = video_capture.read()\n",
        "        if not success or frame is None:  # Stop if the frame cannot be read\n",
        "            break\n",
        "        frame_count += 1\n",
        "\n",
        "        # Save the original frame\n",
        "        original_path = os.path.join(base_out_dir, f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(original_path, frame)\n",
        "\n",
        "        # Apply and save each effect\n",
        "        flip_frame = apply_flip(frame)\n",
        "        flip_path = os.path.join(effect_dirs[\"flip\"], f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(flip_path, flip_frame)\n",
        "\n",
        "        rotated_frame = apply_rotation(frame, angle)\n",
        "        rotation_path = os.path.join(effect_dirs[\"rotation\"], f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(rotation_path, rotated_frame)\n",
        "\n",
        "        color_shifted_frame = apply_color_shift(frame, color_shift_value)\n",
        "        color_shift_path = os.path.join(effect_dirs[\"color_shift\"], f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(color_shift_path, color_shifted_frame)\n",
        "\n",
        "    # Release the video file resources\n",
        "    video_capture.release()\n",
        "\n",
        "# Paths to dataset and videos\n",
        "file_path = '/content/sign-language-dataset/WLASL_v0.3.json'\n",
        "missing_file_path = '/content/sign-language-dataset/missing.txt'\n",
        "videos_dir = '/content/sign-language-dataset/videos/'\n",
        "\n",
        "# Load the WLASL dataset\n",
        "with open(file_path) as file:\n",
        "    wlasl = json.load(file)\n",
        "\n",
        "# Read the list of missing videos\n",
        "with open(missing_file_path, 'r') as file:\n",
        "    missing_videos = file.read().splitlines()\n",
        "\n",
        "# Create the dataset directory\n",
        "dataset_dir = '/content/datasets'\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Process each class in the dataset\n",
        "for i, class_data in enumerate(wlasl):\n",
        "    if i >= num_words:  # Limit processing to a certain number of classes\n",
        "        break\n",
        "    class_name = class_data['gloss']\n",
        "    print(f\"Processing class: {class_name}, {i}\")\n",
        "\n",
        "    # Process each instance (video) for the current class\n",
        "    for instance in class_data['instances']:\n",
        "        video_id = instance['video_id']\n",
        "        if video_id not in missing_videos:  # Skip missing videos\n",
        "            video_file = os.path.join(videos_dir, f\"{video_id}.mp4\")\n",
        "            split_dir = 'Train' if instance['split'] == 'train' else 'Test'\n",
        "            output_dir = os.path.join(dataset_dir, split_dir, class_name, video_id)\n",
        "            vidToFrame(video_file, output_dir, sequence_length)  # Convert video to frames\n",
        "            print(f\"Processed video {video_id} ({split_dir})\")\n"
      ],
      "metadata": {
        "id": "Pd9cL7Ioe939"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Keypoint extraction"
      ],
      "metadata": {
        "id": "1WZW0DIUfLwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mediapipe models and utilities\n",
        "mp_holistic = mp.solutions.holistic  # Mediapipe holistic model for detecting pose, face, and hands\n",
        "mp_drawing = mp.solutions.drawing_utils  # Utility for drawing landmarks\n",
        "mp_drawing_styles = mp.solutions.drawing_styles  # Utility for styling landmarks\n",
        "\n",
        "# Function to detect and draw landmarks from a video frame using Mediapipe\n",
        "def mediapipe_detection(image, model):\n",
        "    # Convert the image from BGR (OpenCV default) to RGB (Mediapipe requirement)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image.flags.writeable = False  # Mark image as read-only to improve performance\n",
        "    results = model.process(image)  # Process the image to detect landmarks\n",
        "    image.flags.writeable = True  # Allow image modifications again\n",
        "    # Convert the image back to BGR for OpenCV compatibility\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    return image, results  # Return the processed image and detection results\n",
        "\n",
        "# Function to extract keypoints from Mediapipe detection results\n",
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
        "    return np.concatenate([pose, face, lh, rh])\n"
      ],
      "metadata": {
        "id": "F23N9B5kjEBo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import cv2\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "VZM-pUqlfZ1t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Loop through all frames"
      ],
      "metadata": {
        "id": "-P-VtL64lWjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_keypoint_arrays(path, split):\n",
        "    # Get the list of all subdirectories (words) in the specified split folder\n",
        "    selected_words = os.listdir(os.path.join(path, split))\n",
        "\n",
        "    # Create base directories for storing npy arrays\n",
        "    os.makedirs('/content/datasets/npy_arrays', exist_ok=True)\n",
        "    os.makedirs(f'/content/datasets/npy_arrays/{split}', exist_ok=True)\n",
        "    working_path = f'/content/datasets/npy_arrays/{split}'\n",
        "\n",
        "    # Path to the folder containing word subfolders\n",
        "    words_folder = os.path.join(path, split)\n",
        "    selected_words1 = []\n",
        "\n",
        "    # Filter words that have not been processed (not present in the npy folder)\n",
        "    for words1 in selected_words:\n",
        "        npy_fold = os.listdir(working_path)\n",
        "        if words1 not in npy_fold:\n",
        "            selected_words1.append(words1)\n",
        "\n",
        "    # Process each word folder that hasn't been processed yet\n",
        "    for word in tqdm(selected_words1):\n",
        "        word_path = os.path.join(working_path, word)\n",
        "        os.makedirs(word_path, exist_ok=True)\n",
        "        video_files = os.listdir(os.path.join(words_folder, word))\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(word_path, video_file)\n",
        "            os.makedirs(video_path, exist_ok=True)\n",
        "            video = sorted(os.listdir(os.path.join(words_folder, word, video_file)))\n",
        "\n",
        "            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "                frame_count = 0\n",
        "                for frame in video:\n",
        "                    frame_path = os.path.join(words_folder, word, video_file, frame)\n",
        "                    frame = cv2.imread(frame_path)\n",
        "                    image, results = mediapipe_detection(frame, holistic)\n",
        "                    keypoints = extract_keypoints(results)\n",
        "                    npy_file_path = os.path.join(video_path, f'{frame_count}.npy')\n",
        "                    np.save(npy_file_path, keypoints)\n",
        "\n",
        "                    frame_count += 1\n",
        "\n",
        "            # Pad the remaining frames with zero arrays if the sequence length is not reached\n",
        "            while frame_count < sequence_length:\n",
        "                npy_file_path = os.path.join(video_path, f'{frame_count}.npy')\n",
        "                np.save(npy_file_path, np.zeros(1662))\n",
        "                frame_count += 1"
      ],
      "metadata": {
        "id": "HRrrzxvnfddX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_keypoint_arrays(f'{dataset_dir}','Train/')"
      ],
      "metadata": {
        "id": "TH4KMzUBfjYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_keypoint_arrays(f'{dataset_dir}','Test')"
      ],
      "metadata": {
        "id": "lYe-XhrLfj1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Preprocess Data and Create Labels and Features"
      ],
      "metadata": {
        "id": "qU_EI5Gi1nWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "words = np.array(os.listdir('/content/datasets/Train'))\n",
        "print(words)\n",
        "label_map = {label: num for num, label in enumerate(words)}\n",
        "print(label_map)\n",
        "\n",
        "sequences = []\n",
        "labels = []\n",
        "\n",
        "# Loop through each word in the list of words\n",
        "for word in words:\n",
        "    DATA_PATH = os.path.join('/content/datasets/npy_arrays/Train', word)\n",
        "    videos = os.listdir(DATA_PATH)\n",
        "\n",
        "    # Loop through each video folder in the current word's directory\n",
        "    for video in videos:\n",
        "        # List all sequence files in the current video directory\n",
        "        for sequence in np.array(os.listdir(os.path.join(DATA_PATH, video))).astype(str):\n",
        "            window = []\n",
        "\n",
        "            # Loop through a fixed number of sequence frames (sequence_length is predefined)\n",
        "            for frame_num in range(sequence_length):\n",
        "                # Load the .npy file corresponding to the current frame number\n",
        "                res = np.load(os.path.join(DATA_PATH, video, \"{}.npy\".format(frame_num)))\n",
        "                window.append(res)\n",
        "\n",
        "            sequences.append(window)\n",
        "            labels.append(label_map[word])\n",
        "\n",
        "# Convert sequences and labels into numpy arrays for compatibility with machine learning libraries\n",
        "np.array(sequences).shape\n",
        "np.array(labels).shape\n",
        "X = np.array(sequences)\n",
        "X.shape\n",
        "y = to_categorical(labels).astype(int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
        "y_test.shape\n"
      ],
      "metadata": {
        "id": "2YeZc6j36q7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Build and Train LSTM Neural Network"
      ],
      "metadata": {
        "id": "pL6rh4p51pnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir=log_dir)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(words.shape[0], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "W-YN_IH41s_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}